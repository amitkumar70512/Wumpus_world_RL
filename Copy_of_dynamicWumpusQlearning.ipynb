{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of dynamicWumpusQlearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# =============================================================================\n",
        "# definition of environment and inputs\n",
        "# =============================================================================\n",
        "#the shape of the environment\n",
        "environment_rows = 5 \n",
        "environment_columns = 5\n",
        "\n",
        "start = (0, 0) #start point\n",
        "\n",
        "#hole1 = (2, 0) #hole1 point\n",
        "#hole2 = (2, 1) #hole2 point\n",
        "#wall = (3, 2) #wall point\n",
        "\n",
        "#goal = (4, 4) #goal point\n",
        "\n",
        "episodes = 500 #one sequence of states, actions, and rewards, which ends with a terminal state\n",
        "\n",
        "epsilon = 0.5 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.8 #discount factor for future rewards\n",
        "\n",
        "# =============================================================================\n",
        "# Q-Learning class\n",
        "# =============================================================================\n",
        "\n",
        "dq_values = {}\n",
        "class Q_Learning:\n",
        "    \n",
        "    def __init__(self, environment_rows, environment_columns, start, hole1, hole2,hole3, hole4,  goal, episodes, epsilon, discount_factor):\n",
        "        self.environment_rows = environment_rows\n",
        "        self.environment_columns = environment_columns\n",
        "        self.start = start\n",
        "        self.hole1 = hole1\n",
        "        self.hole2 = hole2\n",
        "        self.hole3 = hole3\n",
        "        self.hole4 = hole4\n",
        "        self.goal = goal\n",
        "        self.episodes = episodes\n",
        "        self.epsilon = epsilon\n",
        "        self.discount_factor = discount_factor\n",
        "        \n",
        "    def set_matrices(self):\n",
        "        #create a 2D numpy array to hold the rewards for each state\n",
        "        self.rewards = np.full((self.environment_rows, self.environment_columns), -1)\n",
        "        self.rewards[self.hole1] = self.rewards[self.hole2]= self.rewards[self.hole3]= self.rewards[self.hole4]  = -100\n",
        "        self.rewards[self.goal] = 100\n",
        "        \n",
        "        #create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a)\n",
        "        \n",
        "\n",
        "        #define actions\n",
        "        #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "        self.actions = ['up', 'right', 'down', 'left']\n",
        "    \n",
        "    #define a function that determines if the specified location is a terminal state\n",
        "    def is_terminal_state(self, current_row_index, current_column_index):\n",
        "        if self.rewards[current_row_index, current_column_index] == -1:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "    def append_state(self,current_row_index, current_column_index):\n",
        "        if current_row_index-1<0:\n",
        "          rewup=-100\n",
        "        else:\n",
        "          rewup= self.rewards[current_row_index-1,current_column_index]\n",
        "        if current_column_index+1>=self.environment_columns:\n",
        "          rewright=-100\n",
        "        else:\n",
        "          rewright=self.rewards[current_row_index,current_column_index+1]\n",
        "        if current_row_index+1>=self.environment_rows:\n",
        "          rewdown=-100\n",
        "        else:\n",
        "          rewdown=self.rewards[current_row_index+1,current_column_index]\n",
        "        if current_column_index-1<0:\n",
        "          rewleft=-100\n",
        "        else:\n",
        "          rewleft= self.rewards[current_row_index,current_column_index-1]\n",
        "        \n",
        "        state=(rewup,rewright,rewdown,rewleft)\n",
        "        if state not in dq_values:     #if state not present add in table\n",
        "              dq_values.update({state:[0,0,0,0]})\n",
        "        return state\n",
        "\n",
        "    #define an epsilon greedy algorithm that will choose which action to take next\n",
        "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
        "        #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
        "        #then choose the most promising value from the Q-table for this state.\n",
        "        \n",
        "        state=self.append_state(current_row_index,current_column_index)\n",
        "        if np.random.random() < epsilon:   #exploit\n",
        "            return np.argmax(dq_values[state])\n",
        "        else:                              #explore\n",
        "            return np.random.randint(4)\n",
        "\n",
        "    #define a function that will get the next location based on the chosen action\n",
        "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
        "        new_row_index = current_row_index\n",
        "        new_column_index = current_column_index\n",
        "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
        "            new_row_index -= 1\n",
        "        elif self.actions[action_index] == 'right' and current_column_index < self.environment_columns - 1:\n",
        "            new_column_index += 1\n",
        "        elif self.actions[action_index] == 'down' and current_row_index < self.environment_rows - 1:\n",
        "            new_row_index += 1\n",
        "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
        "            new_column_index -= 1\n",
        "        return new_row_index, new_column_index\n",
        "\n",
        "\n",
        "    #run through 500 training episodes\n",
        "    def train(self):\n",
        "        for episode in range(self.episodes):\n",
        "            #get the starting location for this episode\n",
        "            row_index, column_index = self.start\n",
        "\n",
        "            #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "            #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "            while not self.is_terminal_state(row_index, column_index):\n",
        "      \n",
        "                #choose which action to take (i.e., where to move next)\n",
        "                action_index = self.get_next_action(row_index, column_index, self.epsilon)\n",
        "\n",
        "                #store the old row and column indexes\n",
        "                old_row_index, old_column_index = row_index, column_index \n",
        "    \n",
        "                #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
        "    \n",
        "                #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "                reward = self.rewards[row_index, column_index]\n",
        "\n",
        "                #new_q_value = reward + (self.discount_factor * np.max(self.q_values[row_index, column_index]))\n",
        "                state=self.append_state(row_index, column_index)\n",
        "                new_q_value= reward + (self.discount_factor * np.max(dq_values[state]))\n",
        "                state_prev=self.append_state(old_row_index, old_column_index)\n",
        "\n",
        "                #update the Q-value for the previous state and action pair\n",
        "                dq_values[state_prev][action_index] = new_q_value\n",
        "        print(dq_values)\n",
        "        print(len(dq_values))\n",
        "        print(\"----------------------\")   \n",
        "\n",
        "\n",
        "    def get_shortest_path(self, start):\n",
        "        start_row_index, start_column_index = start\n",
        "        #return immediately if this is an invalid starting location\n",
        "        if self.is_terminal_state(start_row_index, start_column_index):\n",
        "            return []\n",
        "        else: #if this is a 'legal' starting location\n",
        "            current_row_index, current_column_index = start_row_index, start_column_index\n",
        "            shortest_path = []\n",
        "            shortest_path.append([current_row_index, current_column_index])\n",
        "            #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
        "            while not self.is_terminal_state(current_row_index, current_column_index):\n",
        "                #get the best action to take\n",
        "                action_index = self.get_next_action(current_row_index, current_column_index, 1)\n",
        "                #move to the next location on the path, and add the new location to the list\n",
        "                current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index, action_index)\n",
        "                print(current_row_index,current_column_index)\n",
        "\n",
        "                shortest_path.append([current_row_index, current_column_index])\n",
        "        return shortest_path\n",
        "\n",
        "\n",
        "    def get_board_path(self,start,shortest_path):\n",
        "      board=self.rewards\n",
        "      for moves in shortest_path:\n",
        "        if board[moves[0]][moves[1]]!=100:\n",
        "          board[moves[0]][moves[1]]=2\n",
        "        \n",
        "      for i in range(environment_rows):\n",
        "        for j in range(environment_columns):\n",
        "          if board[i][j]==-1:\n",
        "            print(\"_\",end=' ')\n",
        "          if board[i][j]==-100:\n",
        "            print(\"X\",end=' ')\n",
        "          if board[i][j]==100:\n",
        "            print(\"G\",end=' ')\n",
        "          if board[i][j]==2:\n",
        "            print(\"O\",end=' ') \n",
        "        print(\"\\n\") \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v9O0Lj8e2olD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#     \n",
        "# =============================================================================\n",
        "Q_L = Q_Learning(environment_rows, environment_columns, start, (0,2), (0,3), (1,4),(2,0),(4,4), episodes, epsilon, discount_factor)\n",
        "Q_L.set_matrices()\n",
        "Q_L.rewards\n",
        "Q_L.train()\n",
        "print(\"finished training for 1st case\")\n",
        "# shortest_path=Q_L.get_shortest_path(start)\n",
        "# print(shortest_path)\n",
        "print(\"################################################\")\n",
        "\n",
        "Q_L1 = Q_Learning(environment_rows, environment_columns, start, (2,0), (2,1), (3,2),(0,4) ,(4,4), episodes, epsilon, discount_factor)\n",
        "Q_L1.set_matrices()\n",
        "Q_L1.rewards\n",
        "Q_L1.train()\n",
        "print(\"finished training for 2nd case\")\n",
        "# shortest_path1=Q_L1.get_shortest_path(start)\n",
        "# print(shortest_path1)\n",
        "# print(\"################################################\")\n",
        "\n",
        "Q_L2 = Q_Learning(environment_rows, environment_columns, start, (2,1), (0,2), (3,0), (4,3),(4,4), episodes, epsilon, discount_factor)\n",
        "Q_L2.set_matrices()\n",
        "Q_L2.rewards\n",
        "Q_L2.train()\n",
        "print(\"finished training for 3rd case\")\n",
        "shortest_path2=Q_L2.get_shortest_path(start)\n",
        "\n",
        "print(shortest_path2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJoaW339Xo4i",
        "outputId": "d5ec2c6a-4fb0-42d1-a91c-d6d8a34e66b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(-100, -1, -1, -100): [9.092861440000005, 9.092861440000005, 9.092861440000005, 9.092861440000005], (-100, -100, -1, -1): [-100.0, 4.019431321600004, 6.274289152000005, 4.019431321600004], (-1, -1, -100, -100): [12.616076800000005, 6.274289152000005, -92.72571084799999, 12.616076800000005], (-1, -1, -1, -100): [9.092861440000005, 17.020096000000006, 17.020096000000006, -86.3839232], (-1, -1, -1, -1): [2.2155450572800035, 4.019431321600004, 4.019431321600004, 4.019431321600004], (-100, -1, -1, -1): [-60.99199999999999, 6.274289152000005, 9.092861440000005, 12.616076800000005], (-1, -100, 100, -1): [62.2, 79.0, 100.0, 6.274289152000005], (-1, -100, -100, -1): [0, 0, 0, 0], (-1, -1, -100, -1): [17.020096000000006, 79.0, 48.760000000000005, 48.760000000000005], (-1, -100, -1, -1): [0, 0, 0, 0], (-1, 100, -100, -1): [48.760000000000005, 100.0, 79.0, 62.2]}\n",
            "11\n",
            "----------------------\n",
            "finished training for 1st case\n",
            "################################################\n",
            "{(-100, -1, -1, -100): [9.092861440000005, 29.40640000000001, 12.616076800000005, 12.616076800000005], (-100, -100, -1, -1): [17.020096000000006, -50.239999999999995, 62.2, 17.020096000000006], (-1, -1, -100, -100): [12.616076800000005, 17.020096000000006, -100.0, -36.8], (-1, -1, -1, -100): [48.760000000000005, 79.0, 79.0, -50.239999999999995], (-1, -1, -1, -1): [17.020096000000006, 62.2, 48.760000000000005, 17.020096000000006], (-100, -1, -1, -1): [12.616076800000005, 12.616076800000005, 17.020096000000006, 17.020096000000006], (-1, -100, 100, -1): [62.2, 79.0, 100.0, 62.2], (-1, -100, -100, -1): [0, 0, 0, 0], (-1, -1, -100, -1): [22.52512000000001, 22.52512000000001, -36.8, 12.616076800000005], (-1, -100, -1, -1): [48.760000000000005, 62.2, 79.0, 48.760000000000005], (-1, 100, -100, -1): [62.2, 100.0, 79.0, 12.616076800000005], (-1, -100, -1, -100): [0, 0, 0, 0], (-100, -1, -100, -1): [-50.239999999999995, 79.0, 17.020096000000006, 12.616076800000005]}\n",
            "13\n",
            "----------------------\n",
            "finished training for 2nd case\n",
            "{(-100, -1, -1, -100): [13.056526849034507, 17.570658561293133, 17.570658561293133, 4.244941746705668], (-100, -100, -1, -1): [17.570658561293133, -59.60101748843947, 23.213323201616415, 4.244941746705668], (-1, -1, -100, -100): [12.616076800000005, 17.020096000000006, -100.0, -36.8], (-1, -1, -1, -100): [13.185823236735198, 23.213323201616415, 17.732279045918997, 17.570658561293133], (-1, -1, -1, -1): [9.896931966914618, 64.37341017431332, 30.51918600924843, 30.266654002020516], (-100, -1, -1, -1): [-59.60101748843947, 50.49872813945066, 23.415348807398743, 13.056526849034507], (-1, -100, 100, -1): [72.1284903329854, 90.69693196691463, 118.732279045919, 13.056526849034507], (-1, -100, -100, -1): [20.273463712312658, -20.0, 14.198752942089826, 10.359002353671862], (-1, -1, -100, -1): [23.415348807398743, 39.39898251156053, -75.58465119260126, 17.570658561293133], (-1, -100, -1, -1): [7.246503759811665, 50.49872813945066, 93.9858232367352, 50.49872813945066], (-1, 100, -100, -1): [62.2, 100.0, 79.0, 12.616076800000005], (-1, -100, -1, -100): [0, 0, 0, 0], (-100, -1, -100, -1): [-50.239999999999995, 79.0, 17.020096000000006, 12.616076800000005], (-1, -100, -100, -100): [23.415348807398743, -48.50127186054934, -92.44382281661791, 4.244941746705668], (-100, -1, -100, -100): [-97.52432422498721, 6.917545573531695, 0, 7.28720188293749]}\n",
            "15\n",
            "----------------------\n",
            "finished training for 3rd case\n",
            "0 1\n",
            "1 1\n",
            "1 2\n",
            "1 3\n",
            "1 4\n",
            "2 4\n",
            "3 4\n",
            "4 4\n",
            "[[0, 0], [0, 1], [1, 1], [1, 2], [1, 3], [1, 4], [2, 4], [3, 4], [4, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_L.get_board_path(start,shortest_path)\n",
        "print(\"---------------------\")\n",
        "Q_L1.get_board_path(start,shortest_path1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk0bpu8dXsZ2",
        "outputId": "362625a8-2ccf-47df-f159-9970551c29ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O _ X X _ \n",
            "\n",
            "O O _ _ X \n",
            "\n",
            "_ O _ _ _ \n",
            "\n",
            "_ O _ _ _ \n",
            "\n",
            "_ O O O G \n",
            "\n",
            "---------------------\n",
            "O _ _ _ _ \n",
            "\n",
            "O O O O O \n",
            "\n",
            "X X _ _ O \n",
            "\n",
            "_ _ X _ O \n",
            "\n",
            "_ _ _ _ G \n",
            "\n"
          ]
        }
      ]
    }
  ]
}